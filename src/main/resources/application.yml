spring:
  application:
    name: kan-ai-cli
  main:
    banner-mode: off
    web-application-type: none
    allow-circular-references: true
  ai:
    openai:
      api-key: ${OPENAI_API_KEY}
      chat:
        options:
          model: gpt-4o-mini
    ollama:
      base-url: http://localhost:11434
      chat:
        model: llama3.2
    chat:
      client:
        enabled: false
        observations:
          log-prompt: true
      observations:
        include-error-logging: true
        log-completion: true
        log-prompt: true
    vertex:
      ai:
        gemini:
          project-id: roronya-dev-baad2
          location: us-central1
          chat:
            options:
              model: gemini-2.5-flash
#    mcp:
#      client:
#        sse:
#          connections:
#            server1:
#              url: http://localhost:8080


# Tracing configuration
#management:
#  tracing:
#    sampling:
#      probability: 1.0
#    enabled: true
#  otlp:
#    tracing:
#      endpoint: http://localhost:4318/v1/traces

# OpenTelemetry configuration
#otel:
#  exporter:
#    otlp:
#      endpoint: http://localhost:4318
#  traces:
#    exporter: otlp

logging:
  level:
    root: ERROR
