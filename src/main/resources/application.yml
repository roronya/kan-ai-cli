spring:
  application:
    name: kan-ai-cli
  main:
    banner-mode: off
    web-application-type: none
    allow-circular-references: true
  ai:
    openai:
      api-key: ${OPENAI_API_KEY}
      chat:
        options:
          model: gpt-4o-mini
    ollama:
      base-url: http://localhost:11434
      chat:
        model: llama3.2
    chat:
      client:
        enabled: false
        observations:
          log-prompt: true
      observations:
        include-error-logging: true
        log-completion: true
        log-prompt: true
    vertex:
      ai:
        gemini:
          project-id: roronya-dev-baad2
          location: us-central1
          chat:
            options:
              model: gemini-2.5-flash


# Tracing configuration
management:
  tracing:
    sampling:
      probability: 1.0
    enabled: true
  otlp:
    tracing:
      endpoint: http://localhost:4318/v1/traces

# OpenTelemetry configuration
otel:
  exporter:
    otlp:
      endpoint: http://localhost:4318
  traces:
    exporter: otlp

logging:
  level:
    root: ERROR
